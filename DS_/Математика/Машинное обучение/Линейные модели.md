#### Где применяются?
- Задачи ***регрессии*** ($X \rightarrow R$ ) - предсказание некоторого вещественного числа
- Задачи ***классификации*** ($X \rightarrow K \text{, где } K = \{ 0, ..., N\} - \text{классы}$) - определение наиболее вероятного класса
#### Примеры из ML Handbook от ШАД
- Представьте, что у вас есть множество объектов XX, а вы хотели бы каждому объекту сопоставить какое-то значение. К примеру, у вас есть набор операций по банковской карте, а вы бы хотели, понять, какие из этих операций сделали мошенники. Если вы разделите все операции на два класса и нулём обозначите законные действия, а единицей мошеннические, то у вас получится ***простейшая задача классификации***. 
- Представьте другую ситуацию: у вас есть данные геологоразведки, по которым вы хотели бы оценить перспективы разных месторождений. В данном случае по набору геологических данных ваша модель будет, к примеру, оценивать потенциальную годовую доходность шахты. ***Это пример задачи регрессии***. 
Числа, которым мы хотим сопоставить объекты из нашего множества иногда называют таргетами (от английского **target**).
--------
#### Определение
Это простейшие модели, где мы имеем ***объекты x из n признаков и таргет y***. Задачи классификации и регрессии можно сформулировать как поиск отображения из множества объектов $X$ в множество возможных таргетов ($y$). 

*В общем виде линейные модели выглядят так:*
$y=x_1w_1+x_2w_2+x_3w_3+...+x_nw_n + w_0 =$
$=  [x,w]+w_0 = (1,x_1,..x_n)\begin{bmatrix} w_0 \\ w_1 \\. \\ . \\ w_n \\\end{bmatrix}= [x,w]$   -- просто короткая запись
Результатом для ***бинарной классификации*** будет разделяющая плоскость, на класс 0 и класс 1. То есть некое разделяющее правило
![[Pasted image 20241207200744.png]]

#### Как применить линейные модели к нелинейным зависимостям?
Линейные модели могут быть применимы для нелинейных, однако для этого надо добавлять **новые признаки**, например $log(x_1)$ или $x_i^2$ и т.д. Например:
- $y=x_1^2w_1+sgn(x_2)w_2+\sin(x_3)w_3+...+x_nw_n + w_0$

#### Как применить категориальные признаки (цвет, пол и т.д.)
Существует много разных способов преобразования категориальных признаков в численные:
- ***one-hot encoding*** (sklearn.preprocessing.OneHotEncoder) - самое простое что можно придумать
	![[Pasted image 20241207202856.png]]
   Мы превращаем один признак в n признаков, где для каждого объекта (строки) только один из этих признаков равен 1 (остальные равны нулю). ![[Pasted image 20241207202908.png]]
   Однако добавлять для ВСЕХ значений по признаку не надо. Пусть одно значение будет являться "по умолчанию", когда все признаки равны 0![[Pasted image 20241207202925.png]]
- ***Labels encoding*** (sklearn.preprocessing.LabelEncoder) - каждой уникальной категории присваивается уникальный номер. Например, если есть метки «red», «green», и «blue», LabelEncoder может назначить им 0, 1, и 2 соответственно
- Бывают и другие способы обработки, но о них не здесь


### Преимущества линейных моделей
- Простота
- Интерпретируемость
	- К примеру, мы можем достаточно легко судить, как влияют на результат те или иные признаки. Скажем, если вес $w_i$ положителен, то с ростом $x_i$-го признака таргет в случае регрессии будет увеличиваться, а в случае классификации наш выбор будет сдвигаться в пользу одного из классов. Значение весов тоже имеет прозрачную интерпретацию: чем вес $w_i$​ больше, тем «важнее» $x_i$-й признак для итогового предсказания. То есть, если вы построили линейную модель, вы неплохо можете объяснить заказчику те или иные её результаты. Это качество моделей называют **интерпретируемостью**. Оно особенно ценится в индустриальных задачах, цена ошибки в которых высока. Если от работы вашей модели может зависеть жизнь человека, то очень важно понимать, как модель принимает те или иные решения и какими принципами руководствуется. При этом не все методы машинного обучения хорошо интерпретируемы, к примеру, поведение искусственных нейронных сетей или градиентного бустинга интерпретировать довольно сложно.
- 